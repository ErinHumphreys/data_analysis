head(covid_tweets)
# save COVID-19 tweets in CSV file.
save_as_csv(covid_tweets, "data/data_raw/covid_tweets.csv")
## plot the frequency of tweets for each user over time
covid_tweets %>%
dplyr::filter(created_at > "2020-04-30") %>%
dplyr::group_by(screen_name) %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of COVID-19 related tweets posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day from May 2020",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
# Creating a Corpus
covid_corpus <- Corpus(VectorSource(as.vector(covid_tweets$text)))
covid_corpus
# Tidy-Text
tidy_covid_tweets <- covid_tweets %>%
select(created_at,text) %>%
unnest_tokens("word", text)
head(tidy_covid_tweets)
tidy_covid_tweets %>%
count(word) %>%
arrange(desc(n))
head(tidy_covid_tweets)
# Stopwords: Removing stop words such as  “the”, “and”, “bot”, “for”, “is”, etc.
covid_corpus <- tm_map(covid_corpus, removeWords, stopwords("english"))
# Remove stopwords as follows:
data("stop_words")
tidy_covid_tweets <- tidy_covid_tweets %>%
anti_join(stop_words) %>%
filter(!(word == "https" |
word == "rt" |
word == "t.co" |
word == "amp"))
# And now we can repeat the count of top words above:
tidy_covid_tweets %>%
count(word) %>%
arrange(desc(n))
head(tidy_covid_tweets)
# Punctuation: Remove all punctuation marks. This is generally considered important, since to an algorithm the punctuation mark “,” will assume a unique numeric identity.
covid_corpus <- tm_map(covid_corpus, content_transformer(removePunctuation))
# Removing Numbers: In many texts, numbers can carry significant meaning.On the other hand, many numbers add little to the meaning of a text.
covid_corpus <- tm_map(covid_corpus, content_transformer(removeNumbers))
# This tells R to remove all numeric digits and the ‘-’ sign means grep excludes them rather than includes them.
tidy_covid_tweets <- tidy_covid_tweets[-grep("\\b\\d+\\b", tidy_covid_tweets$word),]
# Word Case: Force all text into lower case.
covid_corpus <- tm_map(covid_corpus,  content_transformer(tolower))
# Removing Whitespaces:
covid_corpus <- tm_map(covid_corpus, content_transformer(stripWhitespace))
# Use the gsub function again as follows (s+ describes a blank space)
tidy_covid_tweets$word <- gsub("\\s+","",tidy_covid_tweets$word)
# Stemming: Replace words with its most basic conjugate form.
covid_corpus  <- tm_map(covid_corpus, content_transformer(stemDocument), language = "english")
# Stem the tidytext object with the wordStem function [library(SnowballC)].
tidy_covid_tweets <- tidy_covid_tweets %>%
mutate_at("word", funs(wordStem((.), language="en")))
# Create a document-term matrix from a Corpus object. The end of the code above specifies that we only want to include words that are at least two characters long.
covid_DTM <- DocumentTermMatrix(covid_corpus, control = list(wordLengths = c(2, Inf)))
# View the first five rows of the DTM and two of its columns.
inspect(covid_DTM[1:5,3:8])
# Create a DTM in tidytext.
tidy_covid_DTM <-
tidy_covid_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
media_timelines <- read.csv("data/data_raw/media_timelines.csv", stringsAsFactors = FALSE)
head(media_timelines)
# Tokenising approx. 15 000 tweets from media agencies
tidy_media_timelines <- media_timelines %>%
select(created_at, screen_name,text,favorite_count,retweet_count) %>%
unnest_tokens("word", text)
head(tidy_media_timelines)
# Removing stop words and extra meaningless words
data("stop_words")
tidy_less <- tidy_media_timelines %>%
anti_join(stop_words) %>%
filter(!(word == "https"|
word == "rt" |
word == "t.co" |
word == "amp"))
head(tidy_less)
# Separate date and time
tidy_less <- tidy_less %>%
separate(created_at, into = c("date", "time"), sep = "\\s", extra = "merge") %>%
mutate(date = as.Date(date,"%Y-%m-%d"))
head(tidy_less)
# Save output as a CSV.
write.csv("data_out/media_tweets_tidy")
overall_sentiment <-
tidy_less %>%
inner_join(get_sentiments("bing")) %>%
count(date, sentiment) %>%
spread(sentiment,n, fill = 0)
head(overall_sentiment)
# The below is an illustration of a Negative and Positive on one plot.
overall_sentiment_plot <-
ggplot(overall_sentiment, aes(x = date)) +
geom_line(aes(y = negative),color = "blue") +
geom_line(aes(y = positive), color = "red") +
theme_minimal()+
ylab("Frequency of Positive vs. Negative Words in Media Tweets")+
xlab("Date")
overall_sentiment_plot
# Most common Positive and Negative words for Media tweets
common <-  tidy_less %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment)
head(common)
common %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
# Top 20 most frequently posted words
media_desc <- tidy_less %>%
count(word) %>%
arrange(desc(n))
head(media_desc)
# Select only top words
media_20 <- media_desc[1:20,]
head(media_20)
#create factor variable to sort by frequency
media_desc$word <- factor(media_desc$word, levels = media_desc$word[order(media_desc$n,decreasing=TRUE)])
ggplot(media_20, aes(x=reorder(word, -n), y=n, fill=word))+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Covid-19 related tweets")+
xlab("")+
guides(fill=FALSE)
library(textdata)
overall_nrc <-
tidy_less %>%
inner_join(get_sentiments("nrc")) %>%
count(sentiment) %>%
group_by(sentiment)
head(overall_nrc)
ggplot(overall_nrc, aes(x = reorder(sentiment, -n), y = n, fill = sentiment))+
geom_bar(stat = "identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times emotion appears in overall Media")+
xlab("")+
guides(fill = FALSE)
media_nrc_day <-
tidy_less %>%
inner_join(get_sentiments("nrc")) %>%
count(date,sentiment) #%>%
#group_by(sentiment)
#spread(sentiment,n, fill = 0)
media_nrc_day
ggplot(media_nrc_day, aes(x = date, y = n))+
geom_line(aes(fill = sentiment))+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times sentiment appears in Covid-19 related tweets")+
xlab("")+
guides(fill=FALSE)+
facet_wrap(~sentiment)
# Import COVID-19 tweets
tweets_covid <- read.csv("data/data_raw/covid_tweets.csv", stringsAsFactors = FALSE)
head(tweets_covid)
# Tokenising tweets_covid
tidy_covid<- tweets_covid %>%
select(created_at, screen_name,text,favorite_count,retweet_count) %>%
unnest_tokens("word", text)
head(tidy_covid)
# Separate date and time
tidy_covid <- tidy_covid %>%
separate(created_at, into = c("date", "time"), sep = "\\s", extra = "merge") %>%
mutate(date = as.Date(date,"%Y-%m-%d"))
head(tidy_covid)
# Removing stop words and more meaningless words.
data("stop_words")
tidy_covid<-tidy_covid %>%
anti_join(stop_words) %>%
filter(!(word == "https"|
word == "rt" |
word == "t.co" |
word == "amp" |
word == " ' " ))
head(tidy_covid)
# Negative and positive on one plot
overall_covid <-
tidy_covid %>%
inner_join(get_sentiments("bing")) %>%
count(date, sentiment) %>%
spread(sentiment,n, fill = 0)
head(overall_covid)
# Positive and Negative on one plot.
overall_covid_plot <-
ggplot(overall_covid, aes(x = date)) +
geom_line(aes(y = negative),color ="blue") +
geom_line(aes(y = positive), color = "red") +
theme_minimal()+
ylab("Frequency of Positve vs. Negative Words in Tweets related to Covid-19")+
xlab("Date")
overall_covid_plot
# Top 20 most frequently posted words
covid_desc <- tidy_covid %>%
count(word) %>%
arrange(desc(n))
head(covid_desc)
#select only top words
covid_20<-covid_desc[1:20,]
head(covid_20)
# create factor variable to sort by frequency
covid_desc$word <- factor(covid_desc$word,
levels = covid_desc$word[order(covid_desc$n,decreasing = TRUE)])
ggplot(covid_20, aes(x = reorder(word, -n), y = n, fill = word))+
geom_bar(stat = "identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Covid-19 related tweets")+
xlab("")+
guides(fill = FALSE)
covid_contribution <-  tidy_covid %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment)
head(covid_contribution)
covid_contribution %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
overall_covid_agency <-
tidy_covid %>%
inner_join(get_sentiments("bing")) %>%
count(screen_name,date, sentiment) %>% # Count screen names assigns the number of positive and negative words per day by agency
spread(sentiment,n, fill = 0)
head(overall_covid_agency)
overall_covid_plot <-
ggplot(overall_covid_agency, aes(x = date)) +
geom_line(aes(y= negative),color="blue") +
geom_line(aes(y = positive), color = "red") +
theme_minimal()+
ylab("Frequency of Positve vs. Negative Words related to Covid-19 per Agency")+
xlab("Date") +
facet_wrap(~screen_name, ncol = 2)
overall_covid_plot
covid_nrc <-
tidy_covid %>%
inner_join(get_sentiments("nrc")) %>%
count(sentiment) %>%
group_by(sentiment)
#spread(sentiment,n, fill = 0)
covid_nrc
ggplot(covid_nrc, aes(x=reorder(sentiment, -n), y=n, fill=sentiment))+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times sentiment appears in Covid-19 related tweets")+
xlab("")+
guides(fill=FALSE)
covid_nrc_day <-
tidy_covid %>%
inner_join(get_sentiments("nrc")) %>%
count(date,sentiment)
head(covid_nrc_day)
ggplot(covid_nrc_day, aes(x = date, y = n))+
geom_line(aes(fill = sentiment))+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times sentiment appears in Covid-19 related tweets")+
xlab("")+
guides(fill = FALSE)+
facet_wrap(~sentiment)
covid_desc <- tidy_covid %>%
count(word) %>%
arrange(desc(n))
covid_desc
covid_20<-covid_desc[1:20,]
covid_20
data("tidy_covid_DTM")
covid_topic_model <- LDA(tidy_covid_DTM,
k = 6,
control = list(seed = 321))
covid_top_terms %>%
mutate(term = reorder(term, beta)) %>%
mutate(topic = paste("Topic #", topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal()+
theme(plot.title =
element_text(hjust = 0.5, size=18))+
labs(
title = "Topic Model of Media News Tweets",
caption = "Top Terms by Topic (betas)"
)+
ylab("")+
xlab("")+
coord_flip()
covid_topics <- tidy(covid_topic_model, matrix = "beta")
covid_top_terms <-
covid_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
covid_top_terms %>%
mutate(term = reorder(term, beta)) %>%
mutate(topic = paste("Topic #", topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal()+
theme(plot.title =
element_text(hjust = 0.5, size=18))+
labs(
title = "Topic Model of Media News Tweets",
caption = "Top Terms by Topic (betas)"
)+
ylab("")+
xlab("")+
coord_flip()
tinytex::install_tinytex()
install.packages("rtweet")
library(rtweet)
library(maps)
library(tidytext)
library(tidyverse)
library(textdata)
library(dplyr)
library(tm)
library(SnowballC)
library(topicmodels)
library(stm)
library(tidyr)
library(stringr)
library(ggplot2)
install.packages("tinytex")
library(tinytex)
library(rtweet)
library(maps)
library(tidytext)
library(tidyverse)
library(textdata)
library(dplyr)
library(tm)
library(SnowballC)
library(topicmodels)
library(stm)
library(tidyr)
library(stringr)
library(ggplot2)
library(tinytex)
library(knitr)
library(rtweet)
library(maps)
library(tidytext)
library(tidyverse)
library(textdata)
library(dplyr)
library(tm)
library(SnowballC)
library(topicmodels)
library(stm)
library(tidyr)
library(stringr)
library(ggplot2)
install.packages("knitr")
install.packages("topicmodels")
library(tinytex)
library(knitr)
library(rtweet)
library(maps)
library(tidytext)
library(tidyverse)
library(textdata)
library(dplyr)
library(tm)
library(SnowballC)
library(topicmodels)
library(stm)
library(tidyr)
library(stringr)
library(ggplot2)
tinytex::install_tinytex()
head(findingk2)
write_rds(findingk2, "data/data_out/findingk_covid_tweets.rds")
write.csv(findingk2, 'data/data_out/findingk_covid_tweets.csv)
write_rds(findingk2, "data/data_out/findingk_covid_tweets.rds")
save_as_csv(findingk2, 'data/data_out/findingk_covid_tweets.csv)
write_rds(findingk2, "data/data_out/findingk_covid_tweets.rds")
write.csv(findingk2, "data/data_out/findingk_covid_tweets.csv")
write_rds(findingk2, "data/data_out/findingk_covid_tweets.rds")
save_as_csv(findingk2, "data/data_out/findingk_covid_tweets.csv")
write_rds(findingk2, "data/data_out/findingk_covid_tweets.rds")
save_as_csv(findingk2, "data/data_out/findingk_covid_tweets.csv")
findingk2 <- read_rds("data/data_out/findingk_covid_tweets.rds")
head(findingk)
# Save as RDS file.
write_rds(findingk, "data/data_out/findingk_media_tweets.rds")
# Open RDS file.
findingk <- read_rds("data/data_out/findingk_media_tweets.rds")
head(findThoughts)
covid_topics <- tidy(covid_topic_model, matrix = "beta")
covid_top_terms <-
covid_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(covid_top_terms)
# Save as RDS file.
write_rds(covid_top_terms, "data/data_out/covid_top_terms.rds")
# Open RDS file.
covid_top_terms <- read_rds("data/data_out/covid_top_terms.rds")
# Save as RDS file.
write_rds(covid_desc, "data/data_out/covid_desc.rds")
# Open RDS file.
covid_desc <- read_rds("data/data_out/covid_desc.rds")
# Save as RDS file.
write_rds(covid_nrc_day, "data/data_out/covid_nrc_day.rds")
# Open RDS file.
covid_nrc_day <- read_rds("data/data_out/covid_nrc_day.rds")
# Save as RDS file.
write_rds(covid_nrc, "data/data_out/covid_nrc.rds")
# Open RDS file.
covid_nrc <- read_rds("data/data_out/covid_nrc.rds")
# Save as RDS file.
write_rds(covid_contribution, "data/data_out/covid_contribution.rds")
# Open RDS file.
covid_contribution <- read_rds("data/data_out/covid_contribution.rds")
# Save as RDS file.
write_rds(overall_covid, "data/data_out/overall_covid.rds")
# Open RDS file.
covid_contribution <- read_rds("data/data_out/overall_covid.rds")
# Save as RDS file.
write_rds(media_nrc_day, "data/data_out/media_nrc_day.rds")
# Open RDS file.
covid_contribution <- read_rds("data/data_out/media_nrc_day.rds")
# Save as RDS file.
write_rds(overall_nrc, "data/data_out/overall_nrc.rds")
# Open RDS file.
covid_contribution <- read_rds("data/data_out/overall_nrc.rds")
# Save as RDS file.
write_rds(overall_sentiment, "data/data_out/overall_sentiment.rds")
# Open RDS file.
covid_contribution <- read_rds("data/data_out/overall_sentiment.rds")
# Save output as a CSV.
write.csv("data/data_out/media_tweets_tidy")
# Save output as a CSV.
write.csv("data/data_out/media_tweets_tidy")
# save COVID-19 tweets in CSV file.
save_as_csv(covid_tweets, "data/data_raw/covid_tweets.csv")
# Open COVID-19 tweets in CSV file.
read.csv(covid_tweets, "data/data_raw/covid_tweets.csv")
# Open COVID-19 tweets in CSV file.
read.csv("data/data_raw/covid_tweets.csv")
# Open COVID-19 tweets in CSV file.
read.csv("data/data_raw/covid_tweets.csv", stringsAsFactors = FALSE)
common %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
# Save as RDS file.
write_rds(common, "data/data_out/common.rds")
# Open RDS file.
covid_contribution <- read_rds("data/data_out/common.rds")
# Open RDS file.
common <- read_rds("data/data_out/common.rds")
# Open RDS file.
covid_contribution <- read_rds("data/data_out/overall_nrc.rds")
#create factor variable to sort by frequency
media_desc$word <- factor(media_desc$word, levels = media_desc$word[order(media_desc$n,decreasing=TRUE)])
ggplot(media_20, aes(x=reorder(word, -n), y=n, fill=word))+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Covid-19 related tweets")+
xlab("")+
guides(fill=FALSE)
# Save as RDS file.
write_rds(media_20, "data/data_out/media_20.rds")
# Open RDS file.
media_20 <- read_rds("data/data_out/media_20.rds")
ggplot(overall_nrc, aes(x = reorder(sentiment, -n), y = n, fill = sentiment))+
geom_bar(stat = "identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times emotion appears in overall Media")+
xlab("")+
guides(fill = FALSE)
